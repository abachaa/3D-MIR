{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Generating BiomedCLIP Embeddings"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import SimpleITK as sitk\n",
        "import json\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import pickle\n",
        "import sys\n",
        "import open_clip\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Load Model\n",
        "model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
        "tokenizer = open_clip.get_tokenizer('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)\n",
        "model.to(device)\n",
        "\n",
        "# CT scan dataset\n",
        "class LoadDataset(Dataset):\n",
        "    def __init__(self, path, preprocess_fn):\n",
        "        self.path = path\n",
        "        self.preprocess_fn = preprocess_fn\n",
        "        self.volume = self._load_volume(self.path)        \n",
        "        self.slices = [self.volume[i] for i in range(self.volume.shape[0])]\n",
        "\n",
        "    def _load_volume(self, path):\n",
        "        vol = sitk.ReadImage(path)\n",
        "        vol = sitk.GetArrayFromImage(vol)\n",
        "        return vol\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.slices)\n",
        "    \n",
        "    def _preprocess_image(self, im):\n",
        "        return self.preprocess_fn(Image.fromarray(im))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return {\n",
        "            \"image\": self._preprocess_image(self.slices[index])\n",
        "            }"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1694584711225
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Embeddings with BiomedCLIP"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input Argument \n",
        "# Input Nifti Folder\n",
        "dir_path = os.path.join('')\n",
        "\n",
        "## Output embedding directory\n",
        "output_dir = \"\"\n",
        "os.makedirs(output_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = []\n",
        "for img in os.listdir(dir_path):\n",
        "    if '.amlignore' in img:\n",
        "        pass\n",
        "    elif '.DS_Store' in img:\n",
        "        pass\n",
        "    else:\n",
        "        img_path = os.path.join(dir_path, img)\n",
        "        train_data.append(img_path)\n",
        "\n",
        "for scan_data in tqdm(train_data):    \n",
        "    print(scan_data)\n",
        "    image_path = scan_data\n",
        "    image_name = image_path.split('/')[-1].split('.')[0]\n",
        "\n",
        "    # Load CT scan\n",
        "    volume_path = scan_data\n",
        "\n",
        "    # Create dataloader for the CT scan\n",
        "    dataset = LoadDataset(volume_path, preprocess_val)\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        pin_memory=False,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        drop_last=False,\n",
        "        batch_size=32\n",
        "        )\n",
        "\n",
        "    # Generate embeddings (not normalized)\n",
        "    embds = []\n",
        "    for batch in dataloader:\n",
        "        images = batch[\"image\"].to(device) \n",
        "        embd = model(images)[0].cpu().detach().numpy()\n",
        "        embds.append(embd)\n",
        "    embds = np.concatenate(embds, axis=0)\n",
        "    with open(os.path.join(output_dir, f\"{image_name}.pkl\"), \"wb\") as f:\n",
        "        pickle.dump(embds, f)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1694583242169
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "py38",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}